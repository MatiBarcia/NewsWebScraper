from bs4 import BeautifulSoup
import requests
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import defaultdict
from fastapi import FastAPI, HTTPException
from pymongo import MongoClient
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

app = FastAPI()

with open("tokens.txt", "r") as f:
    lines = f.read().splitlines()
    client_uri = lines[0]
    db_name = lines[1]
    collection_name = lines[2]
    api_token = lines[3]

# MongoDB Atlas configuration
client = MongoClient(client_uri)
db = client[db_name]
collection = db[collection_name]

def summarize_text(text, language, num_sentences=2):
    """
    Summarizes the text in the language and number of sentences given.

    Args:
        text (str): Text to summarize.
        language (str): Text language (en-GB, en-US, es, fr).
        num_sentences (int): Number of sentences in summary.

    Returns:
        str: Summary of the text.
    """
    sentences = sent_tokenize(text)
    
    if len(sentences) < 2:
        return "Text is too short to summarize."
    
    dict_lang = {
        "en-GB": "english",
        "en-US": "english",
        "es": "spanish",
        "fr": "french"
    }
    
    stop_words = set(stopwords.words(dict_lang.get(language)))
    words = word_tokenize(re.sub(r'\W', ' ', text.lower()))
    frequencies = defaultdict(int)

    for word in words:
        if word not in stop_words:
            frequencies[word] += 1

    sentence_score = defaultdict(int)
    for sentence in sentences:
        for word in word_tokenize(sentence.lower()):
            if word in frequencies:
                sentence_score[sentence] += frequencies[word]

    important_sentences = sorted(sentence_score, key=sentence_score.get, reverse=True)
    summary = ' '.join(important_sentences[:num_sentences])

    return summary

def summarize_text_AI(text):
    """
    Summarizes the text given using an AI pretrained model.

    Args:
        text (str): Text to summarize.

    Returns:
        str: Summarized text generated by the AI model.
    """
    # Load model and tokenizer
    model_name = "facebook/bart-large-cnn"
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=api_token)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, token=api_token).to("cuda" if torch.cuda.is_available() else "cpu")

    # Tokenize text
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=1024).to("cuda" if torch.cuda.is_available() else "cpu")

    # Generate summary
    with torch.no_grad():
        outputs = model.generate(inputs["input_ids"], max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)

    # Decode output
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

def fetch_article_content(url):
    """
    Extracts the content from BBC's article for the summary.

    Args:
        url (str): Article URL.

    Returns:
        tuple: Title, content and language.
    """
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')
    
    title = soup.find("h1")
    title_text = title.get_text() if title else "Title not available."
    
    language = soup.html.get("lang")
    
    if "bbc.com" in url:
        if "/sport/" in url:
            article = soup.find_all("div", class_="ssrcss-7uxr49-RichTextContainer e5tfeyi1")
            article_text = ''.join(p.get_text() for p in article) if article else "Content not available."
        elif "/mundo/" in url or "/afrique/" in url:
            article = soup.find_all("div", class_="bbc-19j92fr ebmt73l0")
            article_text = ''.join(p.get_text() for p in article) if article else "Content not available."
        elif "/news/" in url or "/travel/" in url or "/culture/" in url:
            article = soup.find("article")
            article_text = article.get_text() if article else "Content not available."
        elif "/live/" in url:
            article = soup.find("div", class_="ssrcss-1o5f7ft-BulletListContainer e5tfeyi0")
            article_text = article.get_text() if article else "Content not available."
        else:
            raise HTTPException(status_code=404, detail="Unknown BBC URL format.")
            
        return title_text, article_text, language
    else:
        raise HTTPException(status_code=400, detail="Unsupported URL.")

@app.post("/")
async def summarize_article(productUrl: str):
    """
    Endpoint to summarize an article from BBC. Saves the summary in MongoDB.

    Args:
        productUrl (str): Article URL.

    Returns:
        dict: Title and summary of the article.
    """
    existing_article = collection.find_one({"url": productUrl})
    if existing_article:
        return {
            "title": existing_article["title"],
            "summary": existing_article["summary"]
        }
    
    title, article_text, language = fetch_article_content(productUrl)
        
    if article_text == "Content not available.":
        raise HTTPException(status_code=404, detail="Content not found.")

    if "/live/" not in productUrl:
        # summary = summarize_text(article_text, language)
        summary = summarize_text_AI(article_text)
    else: 
        summary = article_text

    new_article = {
        "url": productUrl,
        "title": title,
        "summary": summary
    }
    collection.insert_one(new_article)

    return {
        "title": title,
        "summary": summary
    }